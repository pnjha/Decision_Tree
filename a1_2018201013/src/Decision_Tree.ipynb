{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ssl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import itertools\n",
    "import random\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Structure for Internal Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class decision_node:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        #Attribute name on which data split has taken\n",
    "        self.attribute = \"\"\n",
    "        \n",
    "        #Value on the attribute which is the condition\n",
    "        self.value = \"\"\n",
    "        \n",
    "        #Whether attribute is categorical or numerical\n",
    "        self.feature_type = \"\"\n",
    "        \n",
    "        #Count of number of positive data samples\n",
    "        self.positive = 0\n",
    "        \n",
    "        #Count of number of negative data samples\n",
    "        self.negative = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Structure for Leaf Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class leaf_node:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        #To store final result\n",
    "        self.prediction = 0\n",
    "        \n",
    "        #Count of number of positive data samples\n",
    "        self.positive = 0\n",
    "        \n",
    "        #Count of number of negative data samples\n",
    "        self.negative = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(question):\n",
    "    \n",
    "    #read dataset from file\n",
    "    df = pd.read_csv(\"train.csv\")\n",
    "    \n",
    "    df[\"label\"] = df.left\n",
    "    df = df.drop([\"left\"], axis=1)\n",
    "    df[\"left\"]  = df.label\n",
    "    df = df.drop([\"label\"], axis=1)\n",
    "        \n",
    "#     print(df)    \n",
    "        \n",
    "    if question == \"categorical\":\n",
    "        #For training on only categorical features\n",
    "        df = df.drop([\"satisfaction_level\",\"last_evaluation\",\"number_project\",\"average_montly_hours\",\"time_spend_company\"],axis=1)  \n",
    "\n",
    "#     print(df)        \n",
    "        \n",
    "    #dividing dataset into positive and negative values of the label\n",
    "    pos_df = df.loc[df['left'] == 0]\n",
    "    neg_df = df.loc[df['left'] == 1]\n",
    "    \n",
    "    #spliting positive and negative dataset into randomly 80-20 % split\n",
    "    pos_train_df, pos_test_df = train_test_split(pos_df, 0.2)\n",
    "    neg_train_df, neg_test_df = train_test_split(neg_df, 0.2)\n",
    "    \n",
    "    #merging positive and negative data split so that training and validation dataset contains equal number of positive and negative value of feature label \n",
    "    train_df = pd.concat([pos_train_df, neg_train_df])\n",
    "    test_df = pd.concat([pos_test_df, neg_test_df])\n",
    "    \n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spliting dataset in test and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df is the data frame to split and size is the fraction on dataset in validation set\n",
    "def train_test_split(df, size):\n",
    "    \n",
    "    if isinstance(size, float):\n",
    "        size = round(size * len(df))\n",
    "    \n",
    "    #getting indexes of dataset in a list\n",
    "    indices = df.index.tolist()\n",
    "    \n",
    "    #randomly choosing \"size\" number of indices for validation set\n",
    "    indices = random.sample(population=indices, k=size)\n",
    "\n",
    "    #Creating validation set\n",
    "    validation_df = df.loc[indices]\n",
    "    \n",
    "    #Creating trianing set\n",
    "    train_df = df.drop(indices)\n",
    "    \n",
    "    return train_df, validation_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Tree Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tree(data, impurity_measure = \"gini\", counter = 0, min_rows = 3, max_depth = 8, no_of_node = []):\n",
    "    \n",
    "    #To determine next best split column and its value based on which split has to be done\n",
    "    split_column, split_value = find_split(data,impurity_measure)\n",
    "    \n",
    "    #base condition\n",
    "    if ((split_column == \"\") or check_purity_data(data)) or (len(data) < min_rows) or (counter == max_depth) or (no_of_node[0] <= 0):\n",
    "        \n",
    "        #Creating leaf node object\n",
    "        leaf = leaf_node()\n",
    "        \n",
    "        #positive means employee will leave\n",
    "        #negative means employee will not leave\n",
    "        leaf.prediction, leaf.positive, leaf.negative = classify_data(data)\n",
    "        return leaf\n",
    "    \n",
    "    \n",
    "    #Spliting dataset based on split_column and split_value\n",
    "    left_child_data, right_child_data = split_data(data, split_column, split_value)\n",
    "\n",
    "    feature_name = header_list[split_column]\n",
    "    type_of_feature = type_list[split_column]\n",
    "\n",
    "    #Creating internal node object\n",
    "    node = decision_node()\n",
    "    node.attribute = feature_name\n",
    "    node.feature_type = type_of_feature\n",
    "    node.value = split_value\n",
    "    temp, node.positive, node.negative = classify_data(data)\n",
    "    \n",
    "    #list object to keep track of number of number of nodes in decion tree so far\n",
    "    no_of_node[0] = no_of_node[0]-1\n",
    "    \n",
    "    #Recursive call on left child\n",
    "    left_child = build_tree(left_child_data, impurity_measure ,counter+1, min_rows, max_depth, no_of_node)\n",
    "    \n",
    "    #Recursive call on right child\n",
    "    right_child = build_tree(right_child_data,impurity_measure, counter+1, min_rows, max_depth, no_of_node)\n",
    "\n",
    "    #keeping reference of left and right child\n",
    "    node.left = left_child\n",
    "    node.right = right_child\n",
    "    \n",
    "    return node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dt_classifier(test_row, tree):\n",
    "    \n",
    "    #Checking whether we have reached leaf node\n",
    "    if isinstance(tree, leaf_node):\n",
    "        return tree.prediction\n",
    "\n",
    "    #Extracting feature name, value and feature type\n",
    "    feature_name = tree.attribute\n",
    "    value = tree.value\n",
    "    feature_type = tree.feature_type\n",
    "    \n",
    "    \n",
    "    #For continuous feature type\n",
    "    if feature_type == \"continuous\":\n",
    "        \n",
    "        #Condition to recurse to left child\n",
    "        if test_row[feature_name] <= float(value) and tree.left != None:\n",
    "            return dt_classifier(test_row, tree.left)\n",
    "        \n",
    "        #Condition to recurse to right child\n",
    "        elif tree.right!= None:\n",
    "            return dt_classifier(test_row, tree.right)\n",
    "        \n",
    "        #Condition if both left and right child does not exits and we are not at leaf which is possible in case of pruning\n",
    "        else:\n",
    "            if tree.positive > tree.negative:\n",
    "                return \"1\"\n",
    "            else:\n",
    "                return \"0\"\n",
    "    \n",
    "    elif feature_type == \"categorical\":\n",
    "        \n",
    "        #Condition to recurse to left child\n",
    "        if str(test_row[feature_name]) == value and tree.left != None:\n",
    "            return dt_classifier(test_row, tree.left)\n",
    "        \n",
    "        #Condition to recurse to right child\n",
    "        elif tree.right!= None:\n",
    "            return dt_classifier(test_row, tree.right)\n",
    "        \n",
    "        #Condition if both left and right child does not exits and we are not at leaf which is possible in case of pruning\n",
    "        else:\n",
    "            if tree.positive > tree.negative:\n",
    "                return \"1\"\n",
    "            else:\n",
    "                return \"0\"\n",
    "                          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To check whether the label column contains only one distinct value or not\n",
    "def check_purity_data(data):\n",
    "    \n",
    "    label_column = data[:, -1]\n",
    "    unique_classes = np.unique(label_column)\n",
    "\n",
    "    if len(unique_classes) == 1:\n",
    "        #if label is pure i.e. contains only one distinct value\n",
    "        return True  \n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To find number occurrences of dictinct values in label feature and to return values with maximum occurence and count of all distinct values\n",
    "def classify_data(data):\n",
    "    \n",
    "    #Accessing lebal column\n",
    "    label_column = data[:, -1]\n",
    "    \n",
    "    #Finding unique classes and the count\n",
    "    unique_classes, counts_unique_classes = np.unique(label_column, return_counts=True)\n",
    "\n",
    "    #Finding value with maximum count/occurence\n",
    "    index = counts_unique_classes.argmax()\n",
    "    classification = unique_classes[index]\n",
    "    \n",
    "    positives = 0\n",
    "    negatives = 0\n",
    "    \n",
    "    #Find count of each distinct values in label columns\n",
    "    for item in label_column:\n",
    "        if item == 1:\n",
    "            positives += 1\n",
    "        else:\n",
    "            negatives += 1\n",
    "    \n",
    "    return classification, positives, negatives "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To split data based on a particular unique value of a particular feature\n",
    "def split_data(data, split_column, split_value):\n",
    "    \n",
    "    #Get all the values of the feature\n",
    "    split_column_values = data[:, split_column]\n",
    "    \n",
    "    #Get feature type which can be either continuous or categorical \n",
    "    type_of_feature = type_list[split_column]\n",
    "    \n",
    "    #For continuous feature\n",
    "    if type_of_feature == \"continuous\":\n",
    "        left_data = data[split_column_values <= split_value]\n",
    "        right_data = data[split_column_values >  split_value]\n",
    "    \n",
    "    #For categorical feature\n",
    "    else:\n",
    "        left_data = data[split_column_values == split_value]\n",
    "        right_data = data[split_column_values != split_value]\n",
    "    \n",
    "    return left_data, right_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to find feature and  its unique value which provides best split information gain \n",
    "def find_split(data,impurity_measure):\n",
    "    \n",
    "    #To store all split points possible for the current dataset\n",
    "    split_columns_values = {}\n",
    "    \n",
    "    #To get number of columns in numpy ndarray\n",
    "    no_of_columns = data.shape[1]\n",
    "    \n",
    "    #Looping on all features and storing all unique values as split points except on left feature\n",
    "    for column_index in range(no_of_columns - 1):          \n",
    "    \n",
    "        values = data[:, column_index]\n",
    "        \n",
    "        #To get all the unique values of a feature\n",
    "        unique_values = np.unique(values)\n",
    "        \n",
    "        #To get feature type which can be either categorical or continuous\n",
    "        type_of_feature = type_list[column_index]\n",
    "        \n",
    "        if type_of_feature == \"continuous\":\n",
    "            split_columns_values[column_index] = []\n",
    "            \n",
    "            #For continuous we store the mid point of all consecutive unique value as potential split point\n",
    "            for index in range(len(unique_values)):\n",
    "                if index != 0:\n",
    "                    current_value = unique_values[index]\n",
    "                    previous_value = unique_values[index - 1]\n",
    "                    potential_split = (current_value + previous_value)/2\n",
    "\n",
    "                    split_columns_values[column_index].append(potential_split)\n",
    "                    \n",
    "        #For categorical feature we only take those feature whose number of unique values are greater than one so that we don't recurse on same feature again and again\n",
    "        elif type_of_feature == \"categorical\" and len(unique_values) > 1:\n",
    "            split_columns_values[column_index] = unique_values\n",
    "    \n",
    "    #To get present impurity present in dataset based on label feature\n",
    "    present_impurity = get_impurities(data,impurity_measure)\n",
    "    \n",
    "    current_info_gain = 0\n",
    "    info_gain = float('-inf')\n",
    "    \n",
    "    best_split_column = \"\"\n",
    "    best_split_value = 0\n",
    "    \n",
    "    #To calculate information gain for every feature in potential splits dictionary\n",
    "    for column_index in split_columns_values:\n",
    "        \n",
    "        #For every unique value of a feature\n",
    "        for value in split_columns_values[column_index]:\n",
    "            \n",
    "            #spliting data into two part\n",
    "            left_child_data, right_child_data = split_data(data, split_column=column_index, split_value=value)\n",
    "            \n",
    "            #Calculating impurity based on impurity measure\n",
    "            if impurity_measure == \"entropy\":\n",
    "                current_overall_impurity = calculate_entropy(left_child_data, right_child_data)\n",
    "            elif impurity_measure == \"gini\":\n",
    "                current_overall_impurity = calculate_gini(left_child_data, right_child_data)\n",
    "            elif impurity_measure == \"misclassification\":\n",
    "                current_overall_impurity = calculate_misclassification(left_child_data, right_child_data)\n",
    "                \n",
    "            #Calculating information gain    \n",
    "            current_info_gain = present_impurity - current_overall_impurity    \n",
    "                \n",
    "            #Storing feature name and feature value which provides maximum information gain    \n",
    "            if current_info_gain >= info_gain:\n",
    "                info_gain = current_info_gain\n",
    "                best_split_column = column_index\n",
    "                best_split_value = value\n",
    "    \n",
    "    return best_split_column, best_split_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To get impurity of dataset based on a particular impurity measure\n",
    "def get_impurities(data, impurity_measure):\n",
    "    \n",
    "    if impurity_measure == \"entropy\":\n",
    "        return entropy(data)\n",
    "    elif impurity_measure == \"gini\":\n",
    "        return gini(data)\n",
    "    elif impurity_measure == \"misclassification\":\n",
    "        return misclassification(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To segregate continuous feature and categorical feature based on a threshold value\n",
    "def determine_feature_type(df):\n",
    "    \n",
    "    type_list = []\n",
    "    \n",
    "    #Threshlod value which determines if number of unique values id \n",
    "    unique_values_treshold = 5\n",
    "    for feature in df.columns:\n",
    "        \n",
    "        #We assume that label feature is categorical otherwise it cannot be a classification problem\n",
    "        if feature != \"left\":\n",
    "            \n",
    "            #Find unique values of a features\n",
    "            unique_values = df[feature].unique()\n",
    "            value = unique_values[0]\n",
    "\n",
    "            #if value is string then it has to be categorical or if no of unique values is less than threshold\n",
    "            if (isinstance(value, str)) or (len(unique_values) <= unique_values_treshold):\n",
    "                type_list.append(\"categorical\")\n",
    "            else:\n",
    "                type_list.append(\"continuous\")\n",
    "    \n",
    "    return type_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impurity Measure Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To calculate entropy of dataset based on label feature only\n",
    "def entropy(data):\n",
    "    \n",
    "    #Extracting label feature column\n",
    "    label_column = data[:, -1]\n",
    "    \n",
    "    #Extracting all unique values of label feature column\n",
    "    columns , counts = np.unique(label_column, return_counts=True)\n",
    "\n",
    "    #Calculating probability of each unique value of label feature\n",
    "    probabilities = counts / counts.sum()\n",
    "    \n",
    "    #Calculating entropy for every element in the list and taking there sum\n",
    "    entropy = sum(probabilities * -np.log2(probabilities))\n",
    "     \n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To calculate entropy of data of left data and right data\n",
    "def calculate_entropy(left_data, right_data):\n",
    "    \n",
    "    #To calculate total length of data which is to be split\n",
    "    n = len(left_data) + len(right_data)\n",
    "    \n",
    "    #To calculate weight of left child data\n",
    "    probabilty_left_data = len(left_data)/n\n",
    "    \n",
    "    #To calculate weight of right child data\n",
    "    probabilty_right_data = len(right_data)/n\n",
    "\n",
    "    #To calculate overall entropy of a particular feature based on which data got split\n",
    "    feature_entropy =  (probabilty_left_data * entropy(left_data) + probabilty_right_data * entropy(right_data))\n",
    "    \n",
    "    return feature_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gini Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To calculate gini of dataset based on label feature only\n",
    "def gini(data):\n",
    "\n",
    "    #Extracting label feature column\n",
    "    label_column = data[:, -1]\n",
    "    \n",
    "    #Extracting all unique values of label feature column\n",
    "    columns, counts = np.unique(label_column, return_counts=True)\n",
    "\n",
    "    gini = 1\n",
    "    \n",
    "    #Calculating probability of each unique value of label feature\n",
    "    probabilities = counts / counts.sum()\n",
    "    \n",
    "    #Calculating gini for every element in the list and taking there sum\n",
    "    impurity = sum(probabilities **2)\n",
    "    gini -= impurity\n",
    "        \n",
    "    return gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To calculate gini of data of left data and right data\n",
    "def calculate_gini(left_data, right_data):\n",
    "    \n",
    "    #To calculate total length of data which is to be split\n",
    "    n = len(left_data) + len(right_data)\n",
    "    \n",
    "    #To calculate weight of left child data\n",
    "    probabilty_left_data = len(left_data)/n\n",
    "    \n",
    "    #To calculate weight of right child data\n",
    "    probabilty_right_data = len(right_data)/n\n",
    "\n",
    "    #To calculate overall gini of a particular feature based on which data got split\n",
    "    feature_gini =  (probabilty_left_data * gini(left_data) + probabilty_right_data * gini(right_data))\n",
    "    \n",
    "    return feature_gini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Miscalssification Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To calculate misclassification of dataset based on label feature only\n",
    "def misclassification(data):\n",
    "\n",
    "    #Extracting label feature column\n",
    "    label_column = data[:, -1]\n",
    "    \n",
    "    #Extracting all unique values of label feature column\n",
    "    columns, counts = np.unique(label_column, return_counts=True)\n",
    "\n",
    "    #Calculating probability of each unique value of label feature\n",
    "    probabilities = counts / counts.sum()\n",
    "    \n",
    "    misclassification = 1 - max(probabilities)\n",
    "    \n",
    "    return misclassification\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To calculate misclassification of data of left data and right data\n",
    "def calculate_misclassification(left_data, right_data):\n",
    "    \n",
    "    #To calculate total length of data which is to be split\n",
    "    n = len(left_data) + len(right_data)\n",
    "    \n",
    "    #To calculate weight of left child data\n",
    "    probabilty_left_data = len(left_data)/n\n",
    "    \n",
    "    #To calculate weight of right child data\n",
    "    probabilty_right_data = len(right_data)/n\n",
    "\n",
    "    #To calculate overall misclassification of a particular feature based on which data got split\n",
    "    feature_misclassification =  (probabilty_left_data * misclassification(left_data) + probabilty_right_data * misclassification(right_data))\n",
    "    \n",
    "    return feature_misclassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result Analysis Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_confusion_matrix(df):\n",
    "    \n",
    "    #to count true positive\n",
    "    count_TP = 0\n",
    "    \n",
    "    #to count false positive\n",
    "    count_FP = 0\n",
    "    \n",
    "    #to count false negative\n",
    "    count_FN = 0\n",
    "    \n",
    "    #to count true negative\n",
    "    count_TN = 0\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        if row[\"result\"] == row[\"left\"] and row[\"left\"] == 1:\n",
    "            count_TP += 1\n",
    "        elif row[\"result\"] == row[\"left\"] and row[\"left\"] == 0:\n",
    "            count_TN += 1    \n",
    "        elif row[\"result\"] == 1 and row[\"left\"] == 0:\n",
    "            count_FP += 1\n",
    "        elif row[\"result\"] == 0 and row[\"left\"] == 1:    \n",
    "            count_FN += 1\n",
    "            \n",
    "    print(\"True Positive: \", count_TP)\n",
    "    print(\"True Negative: \", count_TN)\n",
    "    print(\"False Positive: \", count_FP)\n",
    "    print(\"False Negative: \", count_FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1 Score Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_f1_score(df):\n",
    "    \n",
    "    precision = calculate_precision(df)\n",
    "    recall = calculate_recall(df)\n",
    "    \n",
    "    #If recall and precision is both 0 then f1 score is undefined\n",
    "    if precision == 0 or recall == 0:\n",
    "        return 0\n",
    "    \n",
    "    #calculate f1 score\n",
    "    f1_score = 2*((precision*recall)/(precision+recall))\n",
    "\n",
    "    return f1_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(df):\n",
    "    \n",
    "    #mean of all results\n",
    "    accuracy = df[\"correct_result\"].mean()\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_precision(df):\n",
    "\n",
    "    #to count true positive\n",
    "    count_TP = 0\n",
    "    \n",
    "    #to count false positive\n",
    "    count_FP = 0\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        if row[\"result\"] == row[\"left\"] and row[\"left\"] == 1:\n",
    "            count_TP += 1\n",
    "        elif row[\"result\"] == 1 and row[\"left\"] == 0:\n",
    "            count_FP += 1\n",
    "    \n",
    "    #To check whether precision is defined or not. If not then return 0\n",
    "    if count_TP == 0 and count_FP == 0 :\n",
    "        return 0\n",
    "    \n",
    "    precision = (count_TP)/(count_TP + count_FP)\n",
    "    \n",
    "    return precision       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_recall(df):\n",
    "    \n",
    "    #to count true positive\n",
    "    count_TP = 0\n",
    "    \n",
    "    #to count false negative\n",
    "    count_FN = 0\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        if row[\"result\"] == row[\"left\"] and row[\"left\"] == 1:\n",
    "            count_TP += 1\n",
    "        elif row[\"result\"] == 0 and row[\"left\"] == 1:    \n",
    "            count_FN += 1\n",
    "    \n",
    "    #To check whether precision is defined or not. If not then return 0\n",
    "    if count_TP == 0 and count_FN == 0 :\n",
    "        return 0\n",
    "    \n",
    "    recall = (count_TP)/(count_TP + count_FN)\n",
    "    \n",
    "    return recall        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_features():\n",
    "    \n",
    "    #read data from csv file\n",
    "    df = pd.read_csv(\"train.csv\")\n",
    "\n",
    "    # Compute the correlation matrix\n",
    "    corr = df.corr()\n",
    "\n",
    "    # Generate a mask for the upper triangle\n",
    "    mask = np.zeros_like(corr, dtype=np.bool)\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "    sns.heatmap(corr, mask=mask, vmax=.3, center=0,square=True, linewidths=.5, cbar_kws={\"shrink\": .5})    \n",
    "    sns.catplot(y=\"satisfaction_level\", x=\"left\",kind=\"box\", hue=\"number_project\", data=df);\n",
    "    sns.catplot(y=\"satisfaction_level\", x=\"left\", hue=\"time_spend_company\", kind = \"box\",data=df);\n",
    "    sns.catplot(y=\"time_spend_company\", x=\"left\",hue=\"promotion_last_5years\", kind = \"box\",data=df);\n",
    "    sns.catplot(y=\"time_spend_company\", x=\"left\",hue=\"sales\", kind = \"box\",data=df);\n",
    "    sns.catplot(y=\"average_montly_hours\", x=\"left\",hue=\"promotion_last_5years\", kind = \"box\",data=df);\n",
    "    sns.catplot(y=\"average_montly_hours\", x=\"left\",hue=\"promotion_last_5years\", data=df);\n",
    "    sns.catplot(y=\"satisfaction_level\", x=\"left\",hue=\"promotion_last_5years\", data=df);\n",
    "    sns.catplot(y=\"satisfaction_level\", x=\"left\",hue=\"Work_accident\", data=df);\n",
    "    sns.catplot(y=\"satisfaction_level\", x=\"left\", hue=\"time_spend_company\", data=df);\n",
    "    sns.catplot(y=\"satisfaction_level\", x=\"left\", hue=\"number_project\", data=df.query(\"number_project != 3\"));\n",
    "    sns.catplot(y=\"satisfaction_level\", x=\"left\", hue=\"number_project\", data=df);\n",
    "    sns.catplot(y=\"satisfaction_level\", x=\"left\",kind=\"swarm\", hue=\"number_project\",data=df);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error vs Tree Depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We iterate from 1 to max_depth to generate error vs depth plot\n",
    "def error_vs_tree_depth(question, max_depth):\n",
    "    \n",
    "\n",
    "    entropy = []\n",
    "    gini = []\n",
    "    misclassification = []\n",
    "    \n",
    "    Depth = []\n",
    "\n",
    "\n",
    "    for i in range(max_depth):\n",
    "\n",
    "        \n",
    "        Depth.append(i+1)\n",
    "        \n",
    "        #splitting dataset into train and validation set\n",
    "        train_df,validation_df = data_preprocessing(question)\n",
    "        \n",
    "        global header_list, type_list\n",
    "        \n",
    "        #storing cloumn header in a global varibale\n",
    "        header_list = train_df.columns\n",
    "\n",
    "        #Generating feature type for each column\n",
    "        type_list = determine_feature_type(train_df)\n",
    "        \n",
    "        #For entropy\n",
    "        nodes = []\n",
    "        nodes.append(10000) #randomly large value\n",
    "        \n",
    "        #building tree\n",
    "        tree = build_tree(train_df.values,\"entropy\",0,8,i+1,nodes)\n",
    "\n",
    "        #Classifiaction of validation data\n",
    "        validation_df[\"result\"] = validation_df.apply(dt_classifier, args=(tree,), axis=1)\n",
    "        validation_df[\"correct_result\"] = validation_df[\"result\"] == validation_df[\"left\"]\n",
    "        \n",
    "        entropy.append((1 - calculate_accuracy(validation_df)))\n",
    "        \n",
    "        del nodes\n",
    "        \n",
    "        \n",
    "        #For Gini\n",
    "        nodes = []\n",
    "        nodes.append(10000) #randomly large value\n",
    "        \n",
    "        #building tree\n",
    "        tree = build_tree(train_df.values,\"gini\",0,8,i+1,nodes)\n",
    "\n",
    "        #Classifiaction of validation data\n",
    "        validation_df[\"result\"] = validation_df.apply(dt_classifier, args=(tree,), axis=1)\n",
    "        validation_df[\"correct_result\"] = validation_df[\"result\"] == validation_df[\"left\"]\n",
    "        \n",
    "        gini.append((1 - calculate_accuracy(validation_df)))\n",
    "        \n",
    "        del nodes\n",
    "        \n",
    "        #For Misclassification\n",
    "        \n",
    "        nodes = []\n",
    "        nodes.append(10000) #randomly large value\n",
    "        \n",
    "        #building tree\n",
    "        tree = build_tree(train_df.values,\"misclassification\",0,8,i+1,nodes)\n",
    "\n",
    "        #Classifiaction of validation data\n",
    "        validation_df[\"result\"] = validation_df.apply(dt_classifier, args=(tree,), axis=1)\n",
    "        validation_df[\"correct_result\"] = validation_df[\"result\"] == validation_df[\"left\"]\n",
    "        \n",
    "        misclassification.append((1 - calculate_accuracy(validation_df)))\n",
    "        \n",
    "        del nodes\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    depth = pd.DataFrame(\n",
    "        {'Depth': Depth,\n",
    "         'Entropy': entropy,\n",
    "         'Gini': gini,\n",
    "         'Misclassification': misclassification\n",
    "        })\n",
    "    \n",
    "    #Error visualisation\n",
    "    depth = depth.melt('Depth', var_name='Impurity Measure',  value_name='Error')\n",
    "    depth_graph = sns.factorplot(x=\"Depth\", y=\"Error\", hue='Impurity Measure', data=depth)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error vs Tree Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_vs_tree_nodes(question, n):\n",
    "    \n",
    "#     df = {}\n",
    "    nodes = []\n",
    "\n",
    "    entropy = []\n",
    "    gini = []\n",
    "    misclassification = []\n",
    "    Nodes = []\n",
    "\n",
    "    \n",
    "    for i in range(0,n,100):\n",
    "\n",
    "        Nodes.append(i+1)\n",
    "        \n",
    "        #splitting dataset into train and validation set\n",
    "        train_df,validation_df = data_preprocessing(question)\n",
    "        \n",
    "        global header_list, type_list\n",
    "        \n",
    "        #storing cloumn header in a global varibale\n",
    "        header_list = train_df.columns\n",
    "\n",
    "        #Generating feature type for each column\n",
    "        type_list = determine_feature_type(train_df)\n",
    "        \n",
    "        #For Entropy \n",
    "        nodes = []\n",
    "        nodes.append(i+1)\n",
    "        \n",
    "        #building tree\n",
    "        tree = build_tree(train_df.values,\"entropy\",0,8,400,nodes)\n",
    "        \n",
    "        #Classifiaction of validation data\n",
    "        validation_df[\"result\"] = validation_df.apply(dt_classifier, args=(tree,), axis=1)\n",
    "        validation_df[\"correct_result\"] = validation_df[\"result\"] == validation_df[\"left\"]\n",
    "        \n",
    "        entropy.append((1 - calculate_accuracy(validation_df)))\n",
    "        del nodes\n",
    "\n",
    "        #For Gini\n",
    "        nodes = []\n",
    "        nodes.append(i+1)\n",
    "        \n",
    "        #building tree\n",
    "        tree = build_tree(train_df.values,\"gini\",0,8,400,nodes)\n",
    "        \n",
    "        #Classifiaction of validation data\n",
    "        validation_df[\"result\"] = validation_df.apply(dt_classifier, args=(tree,), axis=1)\n",
    "        validation_df[\"correct_result\"] = validation_df[\"result\"] == validation_df[\"left\"]\n",
    "        \n",
    "        gini.append((1 - calculate_accuracy(validation_df)))\n",
    "        del nodes\n",
    "        \n",
    "        \n",
    "        #For Misclassification\n",
    "        nodes = []\n",
    "        nodes.append(i+1)\n",
    "        \n",
    "        #building tree\n",
    "        tree = build_tree(train_df.values,\"misclassification\",0,8,400,nodes)\n",
    "        \n",
    "        #Classifiaction of validation data\n",
    "        validation_df[\"result\"] = validation_df.apply(dt_classifier, args=(tree,), axis=1)\n",
    "        validation_df[\"correct_result\"] = validation_df[\"result\"] == validation_df[\"left\"]\n",
    "        \n",
    "        misclassification.append((1 - calculate_accuracy(validation_df)))\n",
    "        del nodes\n",
    "        \n",
    "    node = pd.DataFrame(\n",
    "    {'Nodes': Nodes,\n",
    "     'Entropy': entropy,\n",
    "     'Gini': gini,\n",
    "     'Misclassification': misclassification\n",
    "    })\n",
    "    \n",
    "    #Error visualisation\n",
    "    node = node.melt('Nodes', var_name='Impurity Measure',  value_name='Error')\n",
    "    node_graph = sns.factorplot(x=\"Nodes\", y=\"Error\", hue='Impurity Measure', data=node)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error vs Minimum Number of Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_vs_min_rows(question, n):\n",
    "    \n",
    "\n",
    "    entropy = []\n",
    "    gini = []\n",
    "    misclassification = []\n",
    "    Minimum_Rows = []\n",
    "    \n",
    "    \n",
    "    for i in range(0,n,1):\n",
    "        \n",
    "        Minimum_Rows.append(i+1)\n",
    "        \n",
    "        nodes = []\n",
    "        nodes.append(400)\n",
    "        \n",
    "        #splitting dataset into train and validation set\n",
    "        train_df,validation_df = data_preprocessing(question)\n",
    "        \n",
    "        global header_list, type_list\n",
    "        \n",
    "        #storing cloumn header in a global varibale\n",
    "        header_list = train_df.columns\n",
    "\n",
    "        #Generating feature type for each column\n",
    "        type_list = determine_feature_type(train_df)\n",
    "        \n",
    "        #building tree\n",
    "        tree = build_tree(train_df.values,\"entropy\",0,i+1,400,nodes)\n",
    "        \n",
    "        #Classification of validation data\n",
    "        validation_df[\"result\"] = validation_df.apply(dt_classifier, args=(tree,), axis=1)\n",
    "        validation_df[\"correct_result\"] = validation_df[\"result\"] == validation_df[\"left\"]\n",
    "        \n",
    "        entropy.append((1 - calculate_accuracy(validation_df)))\n",
    "        del nodes\n",
    "        \n",
    "        \n",
    "        nodes = []\n",
    "        nodes.append(400)\n",
    "        \n",
    "        #building tree\n",
    "        tree = build_tree(train_df.values,\"gini\",0,i+1,400,nodes)\n",
    "        \n",
    "        #Classification of validation data\n",
    "        validation_df[\"result\"] = validation_df.apply(dt_classifier, args=(tree,), axis=1)\n",
    "        validation_df[\"correct_result\"] = validation_df[\"result\"] == validation_df[\"left\"]\n",
    "        \n",
    "        gini.append((1 - calculate_accuracy(validation_df)))\n",
    "        del nodes\n",
    "        \n",
    "        \n",
    "        nodes = []\n",
    "        nodes.append(400)\n",
    "        \n",
    "        #building tree\n",
    "        tree = build_tree(train_df.values,\"misclassification\",0,i+1,400,nodes)\n",
    "        \n",
    "        #Classification of validation data\n",
    "        validation_df[\"result\"] = validation_df.apply(dt_classifier, args=(tree,), axis=1)\n",
    "        validation_df[\"correct_result\"] = validation_df[\"result\"] == validation_df[\"left\"]\n",
    "        \n",
    "        misclassification.append((1 - calculate_accuracy(validation_df)))\n",
    "        del nodes\n",
    "\n",
    "    min_rows = pd.DataFrame(\n",
    "    {'Minimum_Rows': Minimum_Rows,\n",
    "     'Entropy': entropy,\n",
    "     'Gini': gini,\n",
    "     'Misclassification': misclassification\n",
    "    })\n",
    "    \n",
    "    #Error visualisation\n",
    "    min_rows = min_rows.melt('Minimum_Rows', var_name='Impurity Measure',  value_name='Error')\n",
    "    min_rows_graph = sns.factorplot(x=\"Minimum_Rows\", y=\"Error\", hue='Impurity Measure', data=min_rows)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy, Gini, Misclassification vs Accuracy, Precision, Recall, F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_metrics(question):\n",
    "    \n",
    "    entropy_accuracy = []\n",
    "    gini_accuracy = []\n",
    "    misclassification_accuracy = []\n",
    "    \n",
    "    entropy_precision = []\n",
    "    gini_precision = []\n",
    "    misclassification_precision = []\n",
    "    \n",
    "    entropy_recall = []\n",
    "    gini_recall = []\n",
    "    misclassification_recall = []\n",
    "    \n",
    "    entropy_f1_score = []\n",
    "    gini_f1_score = []\n",
    "    misclassification_f1_score = []\n",
    "    \n",
    "    iterations = []\n",
    "    \n",
    "    nodes = []\n",
    "\n",
    "    for i in range(10):\n",
    "        \n",
    "        iterations.append(i+1)    \n",
    "        \n",
    "        train_df, validation_df = data_preprocessing(question)\n",
    "    \n",
    "        global header_list, type_list\n",
    "        \n",
    "        #storing cloumn header in a global varibale\n",
    "        header_list = train_df.columns\n",
    "\n",
    "        #Generating feature type for each column\n",
    "        type_list = determine_feature_type(train_df)\n",
    "    \n",
    "    \n",
    "        nodes = []\n",
    "        nodes.append(400)\n",
    "        tree = build_tree(train_df.values,\"entropy\",0,3,8,nodes)\n",
    "        del nodes\n",
    "        \n",
    "        #Classifiaction of validation data\n",
    "        validation_df[\"result\"] = validation_df.apply(dt_classifier, args=(tree,), axis=1)\n",
    "        validation_df[\"correct_result\"] = validation_df[\"result\"] == validation_df[\"left\"]\n",
    "        \n",
    "        \n",
    "        entropy_accuracy.append(calculate_accuracy(validation_df))\n",
    "        entropy_precision.append(calculate_precision(validation_df))\n",
    "        entropy_recall.append(calculate_recall(validation_df))\n",
    "        entropy_f1_score.append(calculate_f1_score(validation_df))\n",
    "        \n",
    "        nodes = []\n",
    "        nodes.append(400)\n",
    "        tree = build_tree(train_df.values,\"gini\",0,3,8,nodes)\n",
    "        del nodes\n",
    "        \n",
    "        #Classifiaction of validation data\n",
    "        validation_df[\"result\"] = validation_df.apply(dt_classifier, args=(tree,), axis=1)\n",
    "        validation_df[\"correct_result\"] = validation_df[\"result\"] == validation_df[\"left\"]\n",
    "        gini_accuracy.append(calculate_accuracy(validation_df))\n",
    "        gini_precision.append(calculate_precision(validation_df))\n",
    "        gini_recall.append(calculate_recall(validation_df))\n",
    "        gini_f1_score.append(calculate_f1_score(validation_df))\n",
    "        \n",
    "        nodes = []\n",
    "        nodes.append(400)\n",
    "        tree = build_tree(train_df.values,\"misclassification\",0,3,8,nodes)\n",
    "        del nodes\n",
    "        \n",
    "        #Classifiaction of validation data\n",
    "        validation_df[\"result\"] = validation_df.apply(dt_classifier, args=(tree,), axis=1)\n",
    "        validation_df[\"correct_result\"] = validation_df[\"result\"] == validation_df[\"left\"]\n",
    "        misclassification_accuracy.append(calculate_accuracy(validation_df))\n",
    "        misclassification_precision.append(calculate_precision(validation_df))\n",
    "        misclassification_recall.append(calculate_recall(validation_df))\n",
    "        misclassification_f1_score.append(calculate_f1_score(validation_df))\n",
    "        \n",
    "      \n",
    "    #Creating dataframe for accuracy data\n",
    "    accuracy = pd.DataFrame(\n",
    "    {'Iterations': iterations,\n",
    "     'Entropy': entropy_accuracy,\n",
    "     'Gini': gini_accuracy,\n",
    "     'Misclassification': misclassification_accuracy\n",
    "    })\n",
    "    \n",
    "    #Accuracy visualisation\n",
    "    accuracy = accuracy.melt('Iterations', var_name='Impurity Measure',  value_name='Accuracy')\n",
    "    accuracy_graph = sns.factorplot(x=\"Iterations\", y=\"Accuracy\", hue='Impurity Measure', data=accuracy)\n",
    "    \n",
    "    #Creating dataframe for precision data\n",
    "    precision = pd.DataFrame(\n",
    "    {'Iterations': iterations,\n",
    "     'Entropy': entropy_precision,\n",
    "     'Gini': gini_precision,\n",
    "     'Misclassification': misclassification_precision\n",
    "    })\n",
    "\n",
    "    #Precision visualisation\n",
    "    precision = precision.melt('Iterations', var_name='Impurity Measure',  value_name='Precision')\n",
    "    precision_graph = sns.factorplot(x=\"Iterations\", y=\"Precision\", hue='Impurity Measure', data=precision)\n",
    "    \n",
    "    #Creating dataframe for recall data\n",
    "    recall = pd.DataFrame(\n",
    "    {'Iterations': iterations,\n",
    "     'Entropy': entropy_recall,\n",
    "     'Gini': gini_recall,\n",
    "     'Misclassification': misclassification_recall\n",
    "    })\n",
    "\n",
    "    #Recall visualisation\n",
    "    recall = recall.melt('Iterations', var_name='Impurity Measure',  value_name='Recall')\n",
    "    recall_graph = sns.factorplot(x=\"Iterations\", y=\"Recall\", hue='Impurity Measure', data=recall)\n",
    "    \n",
    "    #Creating dataframe for f1 score data\n",
    "    f1_score = pd.DataFrame(\n",
    "    {'Iterations': iterations,\n",
    "     'Entropy': entropy_f1_score,\n",
    "     'Gini': gini_f1_score,\n",
    "     'Misclassification': misclassification_f1_score\n",
    "    })\n",
    "\n",
    "    #F1 Score visualisation\n",
    "    f1_score = f1_score.melt('Iterations', var_name='Impurity Measure',  value_name='F1 Score')\n",
    "    f1_score_graph = sns.factorplot(x=\"Iterations\", y=\"F1 Score\", hue='Impurity Measure', data=f1_score)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(question):\n",
    "    \n",
    "    #Parameters and there value at which we obtain maximum accuracy \n",
    "    impurity_measure = \"gini\"\n",
    "    min_rows = 8\n",
    "    depth = 8\n",
    "    max_nodes = []\n",
    "    max_nodes.append(400)\n",
    "    \n",
    "    \n",
    "    global header_list, type_list\n",
    "    \n",
    "    #data preprocessing to split dataset to train and validation set\n",
    "    train_df, validation_df = data_preprocessing(question)\n",
    "    \n",
    "    #storing cloumn header in a global varibale\n",
    "    header_list = train_df.columns\n",
    "    \n",
    "    #Generating feature type for each column\n",
    "    type_list = determine_feature_type(train_df)\n",
    "    \n",
    "    #converting pandas data frame to numpy array\n",
    "    data = train_df.values\n",
    "    \n",
    "    #build tree function invocation\n",
    "    tree = build_tree(data,impurity_measure,0,min_rows,depth,max_nodes)\n",
    "    \n",
    "    del max_nodes\n",
    "    \n",
    "    return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(test_file_name,question):\n",
    "    \n",
    "    #Train decision tree\n",
    "    tree = train_model(question)\n",
    "    \n",
    "    #Read test data from file\n",
    "    test_df = pd.read_csv(test_file_name)\n",
    "\n",
    "    #Neccessary preprocessing\n",
    "    test_df[\"label\"] = test_df.left\n",
    "    test_df = test_df.drop([\"left\"], axis=1)\n",
    "    test_df[\"left\"]  = test_df.label\n",
    "    test_df = test_df.drop([\"label\"], axis=1)\n",
    "\n",
    "\n",
    "    if question == \"categorical\":\n",
    "        #For training on only categorical features\n",
    "        test_df = test_df.drop([\"satisfaction_level\",\"last_evaluation\",\"number_project\",\"average_montly_hours\",\"time_spend_company\"],axis=1)  \n",
    "\n",
    "    \n",
    "    #Applying classification\n",
    "    test_df[\"result\"] = test_df.apply(dt_classifier, args=(tree,), axis=1)\n",
    "    test_df[\"correct_result\"] = test_df[\"result\"] == test_df[\"left\"]\n",
    "\n",
    "    #Calculating accuracy\n",
    "    accuracy = calculate_accuracy(test_df)\n",
    "    \n",
    "    #Calculating precision\n",
    "    precision = calculate_precision(test_df)\n",
    "    \n",
    "    #Calculating recall\n",
    "    recall = calculate_recall(test_df)\n",
    "    \n",
    "    #Calculating f1 score\n",
    "    f1_score = calculate_f1_score(test_df)\n",
    "\n",
    "    #Print confusion matrix\n",
    "    print_confusion_matrix(test_df)\n",
    "    \n",
    "    print(\"Accuracy: \",accuracy)\n",
    "    print(\"Precision: \",precision)\n",
    "    print(\"Recall: \",recall)\n",
    "    print(\"F1 Score: \",f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positive:  0\n",
      "True Negative:  62\n",
      "False Positive:  0\n",
      "False Negative:  8\n",
      "Accuracy:  0.8857142857142857\n",
      "Precision:  0\n",
      "Recall:  0.0\n",
      "F1 Score:  0\n"
     ]
    }
   ],
   "source": [
    "evaluation(\"test.csv\",\"categorical\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positive:  6\n",
      "True Negative:  62\n",
      "False Positive:  0\n",
      "False Negative:  2\n",
      "Accuracy:  0.9714285714285714\n",
      "Precision:  1.0\n",
      "Recall:  0.75\n",
      "F1 Score:  0.8571428571428571\n"
     ]
    }
   ],
   "source": [
    "evaluation(\"test.csv\",\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 - 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-2481337ffd46>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcompare_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"all\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-38-46d55ce02948>\u001b[0m in \u001b[0;36mcompare_metrics\u001b[0;34m(question)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mnodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mnodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"misclassification\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mnodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-109f9dd8b6d8>\u001b[0m in \u001b[0;36mbuild_tree\u001b[0;34m(data, impurity_measure, counter, min_rows, max_depth, no_of_node)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m#To determine next best split column and its value based on which split has to be done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0msplit_column\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimpurity_measure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m#base condition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-55843a78cc37>\u001b[0m in \u001b[0;36mfind_split\u001b[0;34m(data, impurity_measure)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;31m#spliting data into two part\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0mleft_child_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_child_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_column\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumn_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;31m#Calculating impurity based on impurity measure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-8dd2020068ae>\u001b[0m in \u001b[0;36msplit_data\u001b[0;34m(data, split_column, split_value)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtype_of_feature\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"continuous\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mleft_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msplit_column_values\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0msplit_value\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mright_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msplit_column_values\u001b[0m \u001b[0;34m>\u001b[0m  \u001b[0msplit_value\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m#For categorical feature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "compare_metrics(\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 - 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 - 5 - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_vs_tree_depth(\"all\",10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 - 5 - 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "error_vs_tree_nodes(\"all\",1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 - 5 - 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_vs_min_rows(\"all\", 15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
